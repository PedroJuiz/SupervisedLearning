{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroJuiz/SupervisedLearning/blob/main/Incremental_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "export_id": 34737,
        "export_uuid": "34366b07-b33f-4326-952d-d0f29a043cdb",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "# Descomenta esta linea para instalar los paquetes\n!pip install -q scikit-learn==0.24.2",
        "meta_block_is_hidden": "False",
        "meta_input_format": 0,
        "meta_is_hidden": "False",
        "meta_output_format": 2,
        "sequence": "10",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "34366b07-b33f-4326-952d-d0f29a043cdb",
        "id": "VW5nzXdTP9XY"
      },
      "outputs": [],
      "source": [
        "# Descomenta esta linea para instalar los paquetes\n",
        "!pip install -q scikit-learn==0.24.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "export_id": 34715,
        "export_uuid": "f53ccc32-59e6-4afa-bd27-99aecb68a44a",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "<center>\n<img src=\"mioti_logo.jpeg\" width=\"200px\"/>\n<p style=\"font-size: 18px\"><b>Machine learning 3</b><br/>Diego García Morate - diegogm@faculty.mioti.es</p>\n</center>\n<br/>\n\n# Challenge S4: Analizando bosques",
        "meta_block_is_hidden": "False",
        "meta_input_format": 1,
        "meta_is_hidden": "True",
        "meta_output_format": 2,
        "sequence": "30",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "f53ccc32-59e6-4afa-bd27-99aecb68a44a",
        "id": "3JElgeRaP9Xa"
      },
      "source": [
        "<center>\n",
        "<img src=\"mioti_logo.jpeg\" width=\"200px\"/>\n",
        "<p style=\"font-size: 18px\"><b>Machine learning 3</b><br/>Diego García Morate - diegogm@faculty.mioti.es</p>\n",
        "</center>\n",
        "<br/>\n",
        "\n",
        "# Challenge S4: Analizando bosques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "export_id": 34716,
        "export_uuid": "9a9215ec-8f3b-4ea1-840a-f426c4c3786e",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "# Objetivos\n\nEl objetivo de este challenge es ser capaz de clasificar un dataset de tipos de suelo mediante las técnicas de aprendizaje incremental vistas en el worksheet.\n\nImportante: sólo hay una única restricción: ¡**No puedes en ningún momento de este challenge cargar el dataset completo en memoria!** (a excepción que se indique lo contrario).",
        "meta_block_is_hidden": "False",
        "meta_input_format": 1,
        "meta_is_hidden": "True",
        "meta_output_format": 2,
        "sequence": "40",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "9a9215ec-8f3b-4ea1-840a-f426c4c3786e",
        "id": "OAAPbl1rP9Xb"
      },
      "source": [
        "# Objetivos\n",
        "\n",
        "El objetivo de este challenge es ser capaz de clasificar un dataset de tipos de suelo mediante las técnicas de aprendizaje incremental vistas en el worksheet.\n",
        "\n",
        "Importante: sólo hay una única restricción: ¡**No puedes en ningún momento de este challenge cargar el dataset completo en memoria!** (a excepción que se indique lo contrario)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "export_id": 34717,
        "export_uuid": "41f7559a-662f-494b-8e9e-f61a46210fdb",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "## Configuración del entorno",
        "meta_block_is_hidden": "False",
        "meta_input_format": 1,
        "meta_is_hidden": "True",
        "meta_output_format": 2,
        "sequence": "50",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "41f7559a-662f-494b-8e9e-f61a46210fdb",
        "id": "qrAxLoTYP9Xc"
      },
      "source": [
        "## Configuración del entorno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "export_id": 34718,
        "export_uuid": "5b43dc32-9fdb-46bd-98d2-c243008f4ff9",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom math import sqrt\nimport sklearn\n\nimport random\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import Perceptron",
        "meta_block_is_hidden": "False",
        "meta_input_format": 0,
        "meta_is_hidden": "False",
        "meta_output_format": 2,
        "sequence": "60",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "5b43dc32-9fdb-46bd-98d2-c243008f4ff9",
        "id": "gWZe73NUP9Xc"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt\n",
        "import sklearn\n",
        "\n",
        "import random\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.linear_model import Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "export_id": 34719,
        "export_uuid": "ff7f1d27-07ae-4d65-b60c-a0dc114d977a",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "## Covertype dataset\n\nEl dataset que vamos a utilizar en este challenge se denomina Covertype dataset (https://archive.ics.uci.edu/ml/datasets/covertype). Este dataset está formado por más de 500.000 observaciones de regiones de 30x30m de bosques en el parque nacional Roosevelt en Colorado.\n\nLas variables de las que está compuesto el dataset según la documentación (`dataset/covtype.info`) son las siguientes:\n\n<pre style=\"font-size:10px\">\nName                                     Data Type    Measurement                       Description\n\nElevation                               quantitative    meters                       Elevation in meters\nAspect                                  quantitative    azimuth                      Aspect in degrees azimuth\nSlope                                   quantitative    degrees                      Slope in degrees\nHorizontal_Distance_To_Hydrology        quantitative    meters                       Horz Dist to nearest surface water features\nVertical_Distance_To_Hydrology          quantitative    meters                       Vert Dist to nearest surface water features\nHorizontal_Distance_To_Roadways         quantitative    meters                       Horz Dist to nearest roadway\nHillshade_9am                           quantitative    0 to 255 index               Hillshade index at 9am, summer solstice\nHillshade_Noon                          quantitative    0 to 255 index               Hillshade index at noon, summer soltice\nHillshade_3pm                           quantitative    0 to 255 index               Hillshade index at 3pm, summer solstice\nHorizontal_Distance_To_Fire_Points      quantitative    meters                       Horz Dist to nearest wildfire ignition points\nWilderness_Area (4 binary columns)      qualitative     0 (absence) or 1 (presence)  Wilderness area designation\nSoil_Type (40 binary columns)           qualitative     0 (absence) or 1 (presence)  Soil Type designation\nCover_Type (7 types)                    integer         1 to 7                       Forest Cover Type designation\n</pre>\n\nUtilizaremos como variable objetivo la última variable `Cover_Type` que indica la categoría de bosque en la que se clasifica ese área:\n\n<pre style=\"font-size:10px\">\nForest Cover Type Classes:\t\n\n1 -- Spruce/Fir\n2 -- Lodgepole Pine\n3 -- Ponderosa Pine\n4 -- Cottonwood/Willow\n5 -- Aspen\n6 -- Douglas-fir\n7 -- Krummholz\n</pre>",
        "meta_block_is_hidden": "False",
        "meta_input_format": 1,
        "meta_is_hidden": "True",
        "meta_output_format": 2,
        "sequence": "70",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "ff7f1d27-07ae-4d65-b60c-a0dc114d977a",
        "id": "0qJZprdmP9Xc"
      },
      "source": [
        "## Covertype dataset\n",
        "\n",
        "El dataset que vamos a utilizar en este challenge se denomina Covertype dataset (https://archive.ics.uci.edu/ml/datasets/covertype). Este dataset está formado por más de 500.000 observaciones de regiones de 30x30m de bosques en el parque nacional Roosevelt en Colorado.\n",
        "\n",
        "Las variables de las que está compuesto el dataset según la documentación (`dataset/covtype.info`) son las siguientes:\n",
        "\n",
        "<pre style=\"font-size:10px\">\n",
        "Name                                     Data Type    Measurement                       Description\n",
        "\n",
        "Elevation                               quantitative    meters                       Elevation in meters\n",
        "Aspect                                  quantitative    azimuth                      Aspect in degrees azimuth\n",
        "Slope                                   quantitative    degrees                      Slope in degrees\n",
        "Horizontal_Distance_To_Hydrology        quantitative    meters                       Horz Dist to nearest surface water features\n",
        "Vertical_Distance_To_Hydrology          quantitative    meters                       Vert Dist to nearest surface water features\n",
        "Horizontal_Distance_To_Roadways         quantitative    meters                       Horz Dist to nearest roadway\n",
        "Hillshade_9am                           quantitative    0 to 255 index               Hillshade index at 9am, summer solstice\n",
        "Hillshade_Noon                          quantitative    0 to 255 index               Hillshade index at noon, summer soltice\n",
        "Hillshade_3pm                           quantitative    0 to 255 index               Hillshade index at 3pm, summer solstice\n",
        "Horizontal_Distance_To_Fire_Points      quantitative    meters                       Horz Dist to nearest wildfire ignition points\n",
        "Wilderness_Area (4 binary columns)      qualitative     0 (absence) or 1 (presence)  Wilderness area designation\n",
        "Soil_Type (40 binary columns)           qualitative     0 (absence) or 1 (presence)  Soil Type designation\n",
        "Cover_Type (7 types)                    integer         1 to 7                       Forest Cover Type designation\n",
        "</pre>\n",
        "\n",
        "Utilizaremos como variable objetivo la última variable `Cover_Type` que indica la categoría de bosque en la que se clasifica ese área:\n",
        "\n",
        "<pre style=\"font-size:10px\">\n",
        "Forest Cover Type Classes:\n",
        "\n",
        "1 -- Spruce/Fir\n",
        "2 -- Lodgepole Pine\n",
        "3 -- Ponderosa Pine\n",
        "4 -- Cottonwood/Willow\n",
        "5 -- Aspen\n",
        "6 -- Douglas-fir\n",
        "7 -- Krummholz\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "export_id": 34720,
        "export_uuid": "d586984f-01cb-446a-8e64-3db619ecfd02",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "# Análisis previo del dataset\n\n\nCarga un fragmento del dataset (`dataset/covtype.csv`), verifica que las variables cargadas coinciden con el archivo de información del dataset.",
        "meta_block_is_hidden": "False",
        "meta_input_format": 1,
        "meta_is_hidden": "True",
        "meta_output_format": 2,
        "sequence": "80",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "d586984f-01cb-446a-8e64-3db619ecfd02",
        "id": "ZV2CQfA9P9Xd"
      },
      "source": [
        "# Análisis previo del dataset\n",
        "\n",
        "\n",
        "Carga un fragmento del dataset (`dataset/covtype.csv`), verifica que las variables cargadas coinciden con el archivo de información del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "export_id": 34721,
        "export_uuid": "c9a45218-cbad-4886-8383-9ec7e1089839",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "",
        "meta_block_is_hidden": "False",
        "meta_input_format": 0,
        "meta_is_hidden": "False",
        "meta_output_format": 2,
        "sequence": "90",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "c9a45218-cbad-4886-8383-9ec7e1089839",
        "id": "IdrJglOfP9Xd",
        "outputId": "2d89bcf2-8304-43d1-9175-6d79085832e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 1012 elementos.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 10000\n",
        "\n",
        "with open('dataset/covtype.csv', 'r') as file:\n",
        "    iterator = pd.read_csv(file, chunksize=batch_size, delimiter=\";\")\n",
        "\n",
        "    for batch in iterator:\n",
        "        print (f'Cargado batch de datos de {len(batch)} elementos.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "export_id": 34722,
        "export_uuid": "0d5283d0-3b3c-4eb7-a929-08e83a1c95e5",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "# Conjunto de entrenamiento y test\n\nDivide los datos en conjunto de entrenamiento y test (70% entrenamiento y 30%). Para ello deberás leer el fichero una sola vez y guardar los resultados en dos ficheros distintos: `train.csv` y `test.csv`.\n\nRecuerda que no puedes cargar los datos en memoria en su totalidad y usar `train_test_split`.",
        "meta_block_is_hidden": "False",
        "meta_input_format": 1,
        "meta_is_hidden": "True",
        "meta_output_format": 2,
        "sequence": "100",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "0d5283d0-3b3c-4eb7-a929-08e83a1c95e5",
        "id": "iaLFgfQkP9Xe"
      },
      "source": [
        "# Conjunto de entrenamiento y test\n",
        "\n",
        "Divide los datos en conjunto de entrenamiento y test (70% entrenamiento y 30%). Para ello deberás leer el fichero una sola vez y guardar los resultados en dos ficheros distintos: `train.csv` y `test.csv`.\n",
        "\n",
        "Recuerda que no puedes cargar los datos en memoria en su totalidad y usar `train_test_split`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "export_id": 34723,
        "export_uuid": "dedd1e39-1ba3-41c2-8801-4d1c7cd8bb7f",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "",
        "meta_block_is_hidden": "False",
        "meta_input_format": 0,
        "meta_is_hidden": "False",
        "meta_output_format": 2,
        "sequence": "110",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "dedd1e39-1ba3-41c2-8801-4d1c7cd8bb7f",
        "id": "mJRPTMs0P9Xe",
        "outputId": "4b087f6c-7c69-4550-95d8-912ca176727a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 10000 elementos.\n",
            "Cargado batch de datos de 1012 elementos.\n",
            "Datos divididos en train.csv y test.csv\n"
          ]
        }
      ],
      "source": [
        "batch_size = 10000\n",
        "train_ratio = 0.7\n",
        "\n",
        "with open('dataset/covtype.csv', 'r') as file:\n",
        "    iterator = pd.read_csv(file, chunksize=batch_size, delimiter=\",\")\n",
        "\n",
        "    with open('train.csv', 'w') as train_file, open('test.csv', 'w') as test_file:\n",
        "        first_batch = True\n",
        "        for batch in iterator:\n",
        "            print(f'Cargado batch de datos de {len(batch)} elementos.')\n",
        "\n",
        "            train_batch = batch.sample(frac=train_ratio, random_state=42)\n",
        "            test_batch = batch.drop(train_batch.index)\n",
        "\n",
        "            if first_batch:\n",
        "                train_batch.to_csv(train_file, index=False, header=True)\n",
        "                test_batch.to_csv(test_file, index=False, header=True)\n",
        "                first_batch = False\n",
        "            else:\n",
        "                train_batch.to_csv(train_file, index=False, header=False, mode='a')\n",
        "                test_batch.to_csv(test_file, index=False, header=False, mode='a')\n",
        "\n",
        "print(\"Datos divididos en train.csv y test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "export_id": 34724,
        "export_uuid": "ea544f0d-c496-4dcc-a80a-c7d1ae4b8903",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "# ¿Qué deberías hacer si quieres hacer que la división entre entrenamiento y test sea estratificado?\n\nTe recuerdo que una división se denomina estratificada cuando garantiza que la distribución de la variable objetivo se mantiene entre el conjunto de entrenamiento y test. \n\nNo hace falta que implementes la función, con explicar como lo resolverías es suficiente.",
        "meta_block_is_hidden": "False",
        "meta_input_format": 1,
        "meta_is_hidden": "True",
        "meta_output_format": 2,
        "sequence": "120",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "ea544f0d-c496-4dcc-a80a-c7d1ae4b8903",
        "id": "EnFEu8DLP9Xf"
      },
      "source": [
        "# ¿Qué deberías hacer si quieres hacer que la división entre entrenamiento y test sea estratificado?\n",
        "\n",
        "Te recuerdo que una división se denomina estratificada cuando garantiza que la distribución de la variable objetivo se mantiene entre el conjunto de entrenamiento y test.\n",
        "\n",
        "No hace falta que implementes la función, con explicar como lo resolverías es suficiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "export_id": 34725,
        "export_uuid": "222c31d5-0791-456d-8001-e4c3f9e99be0",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "",
        "meta_block_is_hidden": "False",
        "meta_input_format": 0,
        "meta_is_hidden": "False",
        "meta_output_format": 2,
        "sequence": "130",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "222c31d5-0791-456d-8001-e4c3f9e99be0",
        "id": "QnsQRzGMP9Xf"
      },
      "source": [
        "Para obtener una división de entrenamiento y test estratificado, mientras se lee el csv por lotes, se debería calcular la distribucion de la variable objetivo en el dataset completo sin cargar los datos en memoria en su totalidad. Esto podría hacerse leyendo linea por linea contando la frecuencia de cada clase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "export_id": 34726,
        "export_uuid": "20dc2501-8460-4817-b30c-938f65ef47fb",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "# Entrenamiento incremental\n\nEntrena de forma incremental un `Perceptron` para este conjunto de datos. El dataset de test lo puedes cargar en memoria en su totalidad. Te recomiendo que utilices funciones al menos con el preprocesamiento para poder reutilizarlo.\n\nIntenta obtener los mejores resultados posibles. Algunas de las técnicas explicadas en el worksheet te ayudarán a obtener mejores resultados.\n\nPor último indica cual sería el porcentaje de acierto del modelo. ¿es único? ¿cambia a lo largo del tiempo?.",
        "meta_block_is_hidden": "False",
        "meta_input_format": 1,
        "meta_is_hidden": "True",
        "meta_output_format": 2,
        "sequence": "140",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "20dc2501-8460-4817-b30c-938f65ef47fb",
        "id": "z3fSRGdmP9Xf"
      },
      "source": [
        "# Entrenamiento incremental\n",
        "\n",
        "Entrena de forma incremental un `Perceptron` para este conjunto de datos. El dataset de test lo puedes cargar en memoria en su totalidad. Te recomiendo que utilices funciones al menos con el preprocesamiento para poder reutilizarlo.\n",
        "\n",
        "Intenta obtener los mejores resultados posibles. Algunas de las técnicas explicadas en el worksheet te ayudarán a obtener mejores resultados.\n",
        "\n",
        "Por último indica cual sería el porcentaje de acierto del modelo. ¿es único? ¿cambia a lo largo del tiempo?."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "export_id": 34727,
        "export_uuid": "42bbefdb-3996-4060-a6b4-90a5f0a12a14",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "",
        "meta_block_is_hidden": "False",
        "meta_input_format": 0,
        "meta_is_hidden": "False",
        "meta_output_format": 2,
        "sequence": "150",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "42bbefdb-3996-4060-a6b4-90a5f0a12a14",
        "id": "qQ668uNgP9Xf"
      },
      "outputs": [],
      "source": [
        "def preprocesar_datos(X, scaler=None):\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        X_procesada = scaler.fit_transform(X)\n",
        "    else:\n",
        "        X_procesada = scaler.transform(X)\n",
        "    return X_procesada, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCjzvAhOP9Xf",
        "outputId": "06f72e86-d647-4bb5-cedf-681ee1885d2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy : 0.4148154947677621\n"
          ]
        }
      ],
      "source": [
        "test_data = pd.read_csv('test.csv', delimiter=\",\")\n",
        "X_test = test_data.drop('Cover_Type', axis=1)\n",
        "y_test = test_data['Cover_Type']\n",
        "\n",
        "X_test, scaler = preprocesar_datos(X_test)\n",
        "\n",
        "perceptron = Perceptron()\n",
        "\n",
        "with open('train.csv', 'r') as file:\n",
        "    iterator = pd.read_csv(file, chunksize=batch_size, delimiter=\",\")\n",
        "\n",
        "    for batch in iterator:\n",
        "        X_train = batch.drop('Cover_Type', axis=1)\n",
        "        y_train = batch['Cover_Type']\n",
        "\n",
        "        X_train, _ = preprocesar_datos(X_train, scaler)\n",
        "\n",
        "        perceptron.partial_fit(X_train, y_train, classes=np.unique(y_test))\n",
        "\n",
        "y_pred = perceptron.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy : {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "export_id": 34728,
        "export_uuid": "931001ca-149a-4975-9dd8-f9371c068d79",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "# Entrenamiento no incremental\n\nCarga el dataset completo en memoria, dividelo en conjunto de entrenamiento y test y entrenalo en un único paso. Calcula el porcentaje de acierto. ",
        "meta_block_is_hidden": "False",
        "meta_input_format": 1,
        "meta_is_hidden": "True",
        "meta_output_format": 2,
        "sequence": "160",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "931001ca-149a-4975-9dd8-f9371c068d79",
        "id": "VQeQ182gP9Xg"
      },
      "source": [
        "# Entrenamiento no incremental\n",
        "\n",
        "Carga el dataset completo en memoria, dividelo en conjunto de entrenamiento y test y entrenalo en un único paso. Calcula el porcentaje de acierto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "export_id": 34729,
        "export_uuid": "581442da-461a-40ce-bed0-133c09f8d328",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "",
        "meta_block_is_hidden": "False",
        "meta_input_format": 0,
        "meta_is_hidden": "False",
        "meta_output_format": 2,
        "scrolled": true,
        "sequence": "170",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "581442da-461a-40ce-bed0-133c09f8d328",
        "id": "EEiR1WrOP9Xg",
        "outputId": "318a68bf-e205-42fc-bd22-866ae8c52886"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 581012 entries, 0 to 581011\n",
            "Data columns (total 55 columns):\n",
            " #   Column                              Non-Null Count   Dtype\n",
            "---  ------                              --------------   -----\n",
            " 0   Elevation                           581012 non-null  int64\n",
            " 1   Aspect                              581012 non-null  int64\n",
            " 2   Slope                               581012 non-null  int64\n",
            " 3   Horizontal_Distance_To_Hydrology    581012 non-null  int64\n",
            " 4   Vertical_Distance_To_Hydrology      581012 non-null  int64\n",
            " 5   Horizontal_Distance_To_Roadways     581012 non-null  int64\n",
            " 6   Hillshade_9am                       581012 non-null  int64\n",
            " 7   Hillshade_Noon                      581012 non-null  int64\n",
            " 8   Hillshade_3pm                       581012 non-null  int64\n",
            " 9   Horizontal_Distance_To_Fire_Points  581012 non-null  int64\n",
            " 10  Wilderness_Area1                    581012 non-null  int64\n",
            " 11  Wilderness_Area2                    581012 non-null  int64\n",
            " 12  Wilderness_Area3                    581012 non-null  int64\n",
            " 13  Wilderness_Area4                    581012 non-null  int64\n",
            " 14  Soil_Type1                          581012 non-null  int64\n",
            " 15  Soil_Type2                          581012 non-null  int64\n",
            " 16  Soil_Type3                          581012 non-null  int64\n",
            " 17  Soil_Type4                          581012 non-null  int64\n",
            " 18  Soil_Type5                          581012 non-null  int64\n",
            " 19  Soil_Type6                          581012 non-null  int64\n",
            " 20  Soil_Type7                          581012 non-null  int64\n",
            " 21  Soil_Type8                          581012 non-null  int64\n",
            " 22  Soil_Type9                          581012 non-null  int64\n",
            " 23  Soil_Type10                         581012 non-null  int64\n",
            " 24  Soil_Type11                         581012 non-null  int64\n",
            " 25  Soil_Type12                         581012 non-null  int64\n",
            " 26  Soil_Type13                         581012 non-null  int64\n",
            " 27  Soil_Type14                         581012 non-null  int64\n",
            " 28  Soil_Type15                         581012 non-null  int64\n",
            " 29  Soil_Type16                         581012 non-null  int64\n",
            " 30  Soil_Type17                         581012 non-null  int64\n",
            " 31  Soil_Type18                         581012 non-null  int64\n",
            " 32  Soil_Type19                         581012 non-null  int64\n",
            " 33  Soil_Type20                         581012 non-null  int64\n",
            " 34  Soil_Type21                         581012 non-null  int64\n",
            " 35  Soil_Type22                         581012 non-null  int64\n",
            " 36  Soil_Type23                         581012 non-null  int64\n",
            " 37  Soil_Type24                         581012 non-null  int64\n",
            " 38  Soil_Type25                         581012 non-null  int64\n",
            " 39  Soil_Type26                         581012 non-null  int64\n",
            " 40  Soil_Type27                         581012 non-null  int64\n",
            " 41  Soil_Type28                         581012 non-null  int64\n",
            " 42  Soil_Type29                         581012 non-null  int64\n",
            " 43  Soil_Type30                         581012 non-null  int64\n",
            " 44  Soil_Type31                         581012 non-null  int64\n",
            " 45  Soil_Type32                         581012 non-null  int64\n",
            " 46  Soil_Type33                         581012 non-null  int64\n",
            " 47  Soil_Type34                         581012 non-null  int64\n",
            " 48  Soil_Type35                         581012 non-null  int64\n",
            " 49  Soil_Type36                         581012 non-null  int64\n",
            " 50  Soil_Type37                         581012 non-null  int64\n",
            " 51  Soil_Type38                         581012 non-null  int64\n",
            " 52  Soil_Type39                         581012 non-null  int64\n",
            " 53  Soil_Type40                         581012 non-null  int64\n",
            " 54  Cover_Type                          581012 non-null  int64\n",
            "dtypes: int64(55)\n",
            "memory usage: 243.8 MB\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('dataset/covtype.csv')\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9WrFkmJP9Xg"
      },
      "outputs": [],
      "source": [
        "X = df.drop('Cover_Type', axis=1)\n",
        "y = df['Cover_Type']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBVzHjjwP9Xg",
        "outputId": "b1f827c9-fa52-4369-9f65-e8d60a163cd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Perceptron()"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "perceptron = Perceptron()\n",
        "perceptron.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARLOffgOP9Xg",
        "outputId": "16a32cb9-2cab-458f-bb5d-1f467cdebebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Porcentaje de acierto: 0.6105539746649532\n"
          ]
        }
      ],
      "source": [
        "y_pred = perceptron.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Porcentaje de acierto: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "export_id": 34730,
        "export_uuid": "d6bcda22-81f5-405b-bfea-b49b187e51a9",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "# Conclusiones\n\nCompara los resultados obtenidos del aprendizaje incremental con el no incremental. Explica las ventajas e inconvenientes de cada uno de los enfoques. \n\nIndica también algún punto de mejora que se ocurra.",
        "meta_block_is_hidden": "False",
        "meta_input_format": 1,
        "meta_is_hidden": "True",
        "meta_output_format": 2,
        "sequence": "180",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "d6bcda22-81f5-405b-bfea-b49b187e51a9",
        "id": "QsZ6bnFYP9Xg"
      },
      "source": [
        "# Conclusiones\n",
        "\n",
        "Compara los resultados obtenidos del aprendizaje incremental con el no incremental. Explica las ventajas e inconvenientes de cada uno de los enfoques.\n",
        "\n",
        "Indica también algún punto de mejora que se ocurra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "export_id": 34731,
        "export_uuid": "73054340-ef53-42fe-9359-5e22caeb2eef",
        "hard_timeout_in_seconds": "660",
        "hide_title_bar": "True",
        "initial_code": "",
        "meta_block_is_hidden": "False",
        "meta_input_format": 0,
        "meta_is_hidden": "False",
        "meta_output_format": 2,
        "sequence": "190",
        "soft_timeout_in_seconds": "600",
        "title": "",
        "uuid": "73054340-ef53-42fe-9359-5e22caeb2eef",
        "id": "PB4uGEu1P9Xh"
      },
      "source": [
        "En este caso, se obtienen mejores resultados cargando el dataset entero en memoria que realizando un entrenamiento incremental. Esto puede deberse a que en el conjunto de train y test del modelo de aprendizaje incremental esten las clases desbalanceadas, debido a que no hemos tenido en cuenta lo comentado previamente en el notebook acerca de división estratificada.\n",
        "A continuacion, se exponen una serie de ventajas e inconvenientes para cada método:\n",
        "\n",
        "#### APRENDIZAJE INCREMENTAL\n",
        "\n",
        "##### Ventajas\n",
        "- Capacidad para manejar grandes conjuntos de datos que no pueden cargarse completamente en la memoria.\n",
        "- Permite actualizar el modelo en tiempo real a medida que llegan nuevos datos sin necesidad de volver a entrenar con todo el conjunto de datos acumulado.\n",
        "- Puede ser más rápido en ciertos casos, ya que el modelo se actualiza con cada lote de datos en lugar de esperar a que se procesen todos los datos.\n",
        "\n",
        "##### Desventajas\n",
        "- La convergencia del modelo puede ser más lenta o menos estable, ya que los pesos se actualizan con cada lote de datos en lugar de minimizar el error en todo el conjunto de datos.\n",
        "- Puede ser más difícil de implementar y gestionar, ya que se deben manejar los lotes de datos y el proceso de actualización del modelo.\n",
        "\n",
        "#### APRENDIZAJE NO INCREMENTAL\n",
        "\n",
        "##### Ventajas\n",
        "- Suele ser más fácil de implementar y gestionar, ya que el modelo se entrena en un único paso utilizando todo el conjunto de datos.\n",
        "- Puede tener una convergencia más rápida y estable, ya que el modelo se optimiza utilizando todo el conjunto de datos en lugar de actualizarse con cada lote de datos.\n",
        "\n",
        "##### Desventajas\n",
        "- No es adecuado para conjuntos de datos muy grandes que no pueden cargarse completamente en la memoria.\n",
        "- No permite actualizar el modelo en tiempo real a medida que llegan nuevos datos; es necesario volver a entrenar el modelo con todo el conjunto de datos acumulado.\n",
        "- Puede ser más lento en ciertos casos, ya que el modelo debe procesar todo el conjunto de datos antes de que se realicen las actualizaciones de los pesos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHlQ5m4SP9Xh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "notebook": {
      "allow_read_only": "False",
      "meta_autorefresh_in_seconds": "0",
      "meta_layout": "[{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_f53ccc32-59e6-4afa-bd27-99aecb68a44a\",\"x\":0,\"y\":30,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_9a9215ec-8f3b-4ea1-840a-f426c4c3786e\",\"x\":0,\"y\":60,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_41f7559a-662f-494b-8e9e-f61a46210fdb\",\"x\":0,\"y\":90,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_5b43dc32-9fdb-46bd-98d2-c243008f4ff9\",\"x\":0,\"y\":120,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_ff7f1d27-07ae-4d65-b60c-a0dc114d977a\",\"x\":0,\"y\":150,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_d586984f-01cb-446a-8e64-3db619ecfd02\",\"x\":0,\"y\":180,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_c9a45218-cbad-4886-8383-9ec7e1089839\",\"x\":0,\"y\":210,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_0d5283d0-3b3c-4eb7-a929-08e83a1c95e5\",\"x\":0,\"y\":240,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_dedd1e39-1ba3-41c2-8801-4d1c7cd8bb7f\",\"x\":0,\"y\":270,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_ea544f0d-c496-4dcc-a80a-c7d1ae4b8903\",\"x\":0,\"y\":300,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_222c31d5-0791-456d-8001-e4c3f9e99be0\",\"x\":0,\"y\":330,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_20dc2501-8460-4817-b30c-938f65ef47fb\",\"x\":0,\"y\":360,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_42bbefdb-3996-4060-a6b4-90a5f0a12a14\",\"x\":0,\"y\":390,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_931001ca-149a-4975-9dd8-f9371c068d79\",\"x\":0,\"y\":420,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_581442da-461a-40ce-bed0-133c09f8d328\",\"x\":0,\"y\":450,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_d6bcda22-81f5-405b-bfea-b49b187e51a9\",\"x\":0,\"y\":480,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_73054340-ef53-42fe-9359-5e22caeb2eef\",\"x\":0,\"y\":510,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_34366b07-b33f-4326-952d-d0f29a043cdb\",\"x\":0,\"y\":30,\"moved\":false,\"static\":false},{\"w\":48,\"h\":30,\"minW\":15,\"minH\":5,\"i\":\"key_block_id_d5e2eec2-d7d3-4c9e-88bc-cea961609450\",\"x\":0,\"y\":60,\"moved\":false,\"static\":false}]",
      "meta_layout_mobile": "[]",
      "meta_refresh_on_load": "False",
      "mode": "S",
      "uuid": "1664dc41-bde3-4aef-b36d-20b694cb704c"
    },
    "resources": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}